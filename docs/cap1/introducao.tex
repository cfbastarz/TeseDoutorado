\chapter{INTRODUÇÃO}
\label{introducao}

Na década de 1950 Jules Charney indicou que mesmo com o contínuo melhoramento dos modelos de Previsão Numérica de Tempo (PNT), ainda sim haveria um limite para a habilidade dos modelos em prever os estados futuros da atmosfera. Este limite estaria então, relacionado a inevitáveis erros e imprecisões de modelagem e à erros nas condições iniciais dos modelos. Durante a década seguinte, Edward Lorenz mostrou que por mais perfeitos que fossem os modelos numéricos de PNT, sempre haveria um limite para a previsão de fenômenos atmosféricos. Ele se referia ao resultados de experimentos com um modelo atmosférico simples de baixa ordem (12 variáveis) em que dadas duas condições iniciais ligeiramente diferentes, seus resultados divergiam drasticamente após duas semanas, sendo tão diferentes quanto duas previsões aleatórias fornecidas pelo modelo. Estes experimentos ficariam conhecidos como ``experimentos gêmeos'', em que dado o mesmo modelo e duas condições iniciais ligeiramente diferentes, após a evolução temporal das equações do modelo, os resultados encontrados seriam completamente distintos. Este tipo de comportamento encontrado nos experimentos gêmeos, foi atribuído por Lorenz as instabilidades inerentes a qualquer sistema de natureza dinâmica não linear, tal como a atmosfera. Originalmente, a ideia de Lorenz era mostrar que previsões estatísticas não tinham capacidade de conferir precisão as previsões de modelos com dinâmica não linear, mas que a previsão numérica de tempo tinha potencial suficiente para ser preditiva através de métodos estatísticos. Através desta experiência, Lorenz descobriu o teorema fundamental da previsibilidade, estabelecendo que ``sistemas instáveis tem um limite de previsibilidade finito (enquanto a estabilidade do fluxo se mantém), enquanto que sistemas estáveis (periódicos ou estacionários, por manterem a estabilidade de seus fluxos), tem um limite de previsibilidade infinito''. Desde as descobertas de Lorenz, a ideia de que a atmosfera se comporta como um sistema dinâmico não linear, sendo ela previsível até alguns poucos dias (e depois disso seu comportamento é caótico, imprevisível), fez com que a ideia de determinação do estado do tempo mudasse. Esta mudança se refletiu na forma como as previsões numéricas de tempo são realizadas, deixando de serem simplesmente determinísticas, passando a serem probabilísticas (ou estocásticas), permitindo-se explorar a natureza caótica da atmosfera e consequentemente a extensão da validade da PNT operacional.

O período de duas semanas observado por Lorenz é o limite prático da previsão numérica de tempo e permanece válido ainda hoje. Naquela época, pouco mais de uma década após a introdução do computador eletrônico para a integração numérica dos modelos de previsão de tempo, a precisão e a habilidade das previsões não era tão alta quando dois dias, ou seja, a validade das previsões daquela época era bastante curta e não alcançava 72 horas. Apesar deste limite encontrado por Lorenz, iniciativas (e.g., \textit{The Subseasonal to Seasonal (S2S) Prediction Project Database}) tem sido colocadas em prática para que seja possível realizar previsões entre as escalas subsazonal e climática \cite{vitaretal/2017}.

A previsibilidade da atmosfera é extremamente dependente da forma como ela mesma evolui, pois em alguns dias a previsão pode se comportar muito bem permanecendo válida por vários dias em sequência, mas em questão de poucas horas, a previsão para o dia seguinte pode ser comprometida. Este comportamento da atmosfera, já apontado por Lorenz como sendo caótico, fez com que fosse necessário também, se considerar a natureza estocástico-dinâmica da atmosfera.

A assimilação de dados, como um conjunto de técnicas para a determinação das análises dos modelos de PNT, tem mostrado entre seus principais avanços, alternativas para a modelagem e a representação das covariâncias dos erros de previsão. Entre estas alternativas, destacam-se a integração entre sistemas de assimilação de dados com diferentes abordagens, levando ao que se denomina de sistemas híbridos, com o objetivo de se combinar as características de diferentes sistemas de assimilação para representar de forma mais adequada a evolução espaço-temporal dos erros das previsões dos modelos, necessárias para a determinação das análises. Esta evolução espaço-temporal, é o que se denomina por ``erros do dia'' e representam uma das principais fontes de incertezas do processo de modelagem como um todo.

Fontes de incerteza são uma característica intrínseca a qualquer sistema dinâmico que apresente comportamento aleatório e relações não lineares entre as suas variáveis de estado. A atmosfera, por exemplo, é um sistema caótico em que diversos parâmetros (como a umidade e a temperatura) podem ser determinados por leis de conservação (ie., conservação de massa e energia) que regem a sua evolução temporal. Outros parâmetros, como os ventos (que são determinados pela lei de conservação do \textit{momentum}) podem ser modelados e a sua dinâmica em larga escala é bem determinada; entretanto, quando se variam as escalas espaciais e temporais destas quantidades, as leis que antes eram aplicadas para a sua determinação, passam a ser incompletas. Isto significa que relações não lineares e o comportamento aleatório destas quantidades passam a ter alguma importância e a sua representação torna-se necessária. Um exemplo deste tipo de fenômeno é o movimento Browniano observado pelo botânico Robert Brown na segunda década do século XIX e descrito pelo físico Albert Einstein no início do século XX.

Dada a complexidade da atmosfera, a modelagem dos processos físicos e a sua representação pelos modelos de PNT é um grande desafio. Muitas fontes de incertezas e aproximações fazem com que a natureza caótica da atmosfera tenha que ser modelada, muitas vezes, utilizando-se métodos probabilísticos ao invés de se tentar alcançar uma solução determinística. Logo, uma aproximação para esta necessidade, envolve um conjunto de soluções, as quais podem ser obtidas mediante a imposição de condições iniciais e de contorno diferentes ou mesmo perturbações aleatórias. Como resultado, obtém-se uma pequena amostra de um enorme conjunto (talvez infinito) de possíveis soluções. Uma segunda aproximação para se abordar esta característica caótica dos sistemas dinâmicos (tal como a atmosfera), é a combinação de diferentes modelos (que não necessariamente possuem diferentes condições iniciais) para tentar gerar uma solução que seja uma amostra mais representativa do universo das possíveis soluções. Neste contexto, na assimilação de dados, os sistemas híbridos tem sido explorados durante os últimos anos com o objetivo de tentar reproduzir em um problema determinístico, a característica caótica e evolutiva que os erros associados a modelagem possuem e que são fundamentais para a determinação dos campos de análise. 

\section{Motivação}
\label{motivacao}

O Centro de Previsão de Tempo e Estudos Climáticos (CPTEC) do Instituto Nacional de Pesquisas Espaciais (INPE), desde meados do ano de 2010, desenvolveu algumas aplicações com alguma variação do filtro de Kalman por conjunto utilizando as previsões do Modelo de Circulação Geral da Atmosfera (MCGA-CPTEC/INPE). Algumas destas aplicações, foram baseadas no \textit{Local Ensemble Transform Kalman Filter} (LETKF) \cite{huntetal/2007}, valendo-se de vantagens como baixo custo computacional e fácil implementação. Estas características tornam o LETKF uma opção atrativa para estudos acadêmicos e aplicações com diversos modelos. Entretanto, para estas aplicações com o LETKF no CPTEC, limitações estiveram presentes no tratamento das observações não convencionais e outras fontes de dados observacionais, as quais demandariam o desenvolvimento de operadores específicos e outras rotinas (e.g., ciclo de assimilação de dados) para apenas então viabilizar uma possível aplicação operacional robusta. 

\citeonline{hoffmanetal/2008} desenvolveram com o LETKF no CPTEC o \textit{CPTEC Ocean Data Assimilation System} (CODAS). Em estudos preliminares, os autores mostraram que a análise oceânica com até 12 membros do CODAS utilizando como conjunto de previsões as previsões do modelo \textit{Modular Ocean Model 4} (MOM4), se mostrou bastante próxima a realidade (neste estudo o modelo foi considerado como perfeito). \citeonline{aravequia/2008} realizou estudos com a assimilação de radiâncias do sensor \textit{Atmospheric InfraRed Sounder} (AIRS) utilizando o modelo de transferência radiativa \textit{Community Radiative Transfer Model} (CRTM) do \textit{Joint Center for Satellite Data Assimilation} da \textit{National Oceanic and Atmospheric Administration} (JCSDA/NOAA) e encontrou que a assimilação das medidas diretas de radiâncias exercem um impacto positivo sobre a América do Sul. \citeonline{souzaetal/2010} realizaram comparações entre dois esquemas de convecção \textit{cumulus} (Grell e Kuo), do MCGA-CPTEC/INPE e qual o seu impacto para as análises do LETKF, utilizando as previsões de curto prazo do modelo na resolução TQ0126L028. Os resultados mostraram que com o esquema de Grell houve redução do erro quadrático médio em relação as análises do \textit{National Centers for Environmental Predictions }(NCEP). \citeonline{cintra/2010} simularam o LETKF utilizando redes neurais para o modelo \textit{Simplified Parameterizations, privitivE-Equation DYnamics} (SPEEDY) \cite{molteni/2003}. Neste trabalho, foi encontrado que as redes neurais conseguiram simular as principais características do filtro, com a principal vantagem de ser computacionalmente ainda mais barato. \citeonline{medeiros/2011} realizou um estudo para avaliar o impacto da assimilação de dados de radiâncias na temperatura do ar produzida pela análise do LETKF. \citeonline{diniz/2012}, realizou um estudo sobre o impacto que os diversos tipos de observações disponíveis para o LETKF exercem nas previsões de curto prazo produzidas pelo MCGA-CPTEC/INPE a partir das análises do LETKF. \citeonline{avancoetal/2013} e \citeonline{sapuccietal/2016} desenvolveram um operador observação baseado no software \textit{Radio Occultation Processing Package} (ROPP) para a assimilação de dados de \textit{Radio Occultation-Global Positioning System} (ROGPS) para o LETKF e mostraram que a inclusão destes dados exerce impacto positivo nas análises do sistema sobre a América do Sul. 

Mais recentemente, no CPTEC tem-se aplicado o sistema \textit{Gridpoint Statistical Interpolation} (GSI) \cite{wuetal/2002,kleistetal/2009}, para gerar análises em escala global utilizando o MCGA-CPTEC/INPE e também (a partir de 2016), o modelo \textit{Brazilian Atmospheric Model} (BAM). Diferente dos sistemas de assimilação de dados que utilizam o filtro de Kalman por conjunto, o GSI é um sistema variacional que minimiza uma função custo para se determinar o estado da análise. Diversos trabalhos com estudos de impacto e diagnósticos foram realizados com o sistema GSI acoplado com o MCGA-CPTEC/INPE (sendo denominado de G3DVAR) e que foram importantes também na manutenção do sistema operacional. \citeonline{azevedo/2014}, aplicou a técnica de \textit{Observing System Experiments} (OSE) para avaliar o impacto dos diferentes tipos de sistemas de observações nas análises e previsões do sistema. \citeonline{araujo/2015} estudou a parametrização do comprimento de rugosidade térmica, um importante parâmetro na determinação da temperatura da superfície terrestre, utilizando o modelo \textit{Simplified Simple Biosphere} (SSiB) no sistema G3DVAR. \citeonline{penna/2015} avaliou a sensibilidade dos canais sensíveis a superfície terrestre, utilizando as radiâncias do AMSU/A a partir dos satélites 15 e 19 da série NOAA. \citeonline{rodrigues/2017} realizou um estudo com o sistema G3DVAR para avaliar o impacto da injunção de umidade, um importante característica do sistema de assimilação GSI que permite controlar soluções não físicas da umidade (negativa e supersaturada).

Um sistema sequencial baseado no filtro de Kalman por conjunto e um sistema variacional em 3 dimensões, são os componentes necessários para um sistema híbrido de assimilação de dados que possa ser utilizado operacionalmente em um futuro próximo pelo CPTEC. Além disso, nos principais centros operacionais de PNT e assimilação de dados (e.g., NCEP, \textit{European Centre for Medium Range Weather Forecasts} (ECMWF) e \textit{Meteorological Office} (MetOffice), \textit{Deutsche Wetterdienst} (DWD), \textit{Hydrometeorological Center of Russia} (RHMC) e outros), já desenvolveram ou estão desenvolvendo sistemas híbridos \cite{wgne27/2011,wgne28/2012,wgne30/2015}. Durante o \textit{Sixth Symposium on Data Assimilation} realizado pela WMO nos Estados Unidos (em outubro de 2013), ficou bastante evidente a tendência pela escolha dos centros operacionais por sistemas híbridos, cujas vantagens já haviam sido evidenciadas principalmente nos trabalhos de \citeonline{wangetal/2007,wangetal/2008a,wangetal/2008b}, sendo que boa parte dos resultados mostrados se referem a um conjunto de previsões provenientes do sistema baseado em 4DVar. Os motivos pelos quais os centros operacionais seguem esta tendência, em resumo, são os seguintes:

\begin{enumerate}
    \item Atualmente, o limite técnico dos sistemas operacionais de assimilação de dados está na capacidade computacional, o qual limita também o aproveitamento do crescente número de observações não convencionais (i.e., radiâncias);
    \item Tendo-se em vista este cenário, sabe-se que para calcular a análise pela abordagem variacional, é necessária a inversão da matriz de covariâncias dos erros de previsão. Devido a quantidade de variáveis e pontos de grade representando os estados do modelo, é praticamente impossível armazenar a matriz de covariâncias por completo. Por isso, ela é simplificada em sua representação atual e deve ser pré-condicionada para a minimização da função custo;
    \item Uma abordagem alternativa para se representar as covariâncias dos erros de previsão é através de técnicas estatísticas e uma técnica que permite esta representação é o filtro de Kalman por conjunto (e suas variações);
    \item Sistemas variacionais tratam as covariâncias dos erros de previsão como sendo informações estáticas (considerando que os erros crescem linearmente nas primeiras 48 horas de previsão, e.g., \citeonline{zhangekrishnamurti/1999}, e que sua distribuição é Gaussiana), o que implica na falta de habilidade do sistema em incorporar as variações diárias dos erros associados as previsões e esta importante informação que deixa de ser utilizada pela análise;
    \item Sistemas sequenciais baseados no filtro de Kalman por conjunto tem a característica de fornecer uma estimativa dos ``erros do dia'', mas podem ser computacionalmente caros, além do que há que se lidar com conjuntos de análises/previsões relativamente grandes (e.g., 40 membros ou mais) quando se trabalha com sistemas caóticos (como a atmosfera) e, dificuldades com estes conjuntos podem incluir problemas de amostragem (\textit{undersampling}), crescimento inapropriado das perturbações (\textit{imbreeding}), divergência do filtro e correlações espúrias;
    \item Através da aplicação de sistemas híbridos, mostra-se que é possível incluir a estimativa dos ``erros do dia'' dentro do ciclo de assimilação de dados variacional (i.e., completando a informação representada na matriz de covariâncias estática) ponderando-se a contribuição das covariâncias dos erros de previsão da parte estática/estacionária (provenientes do sistema variacional) e dinâmico/não estacionária (provenientes do sistema baseado no filtro de Kalman por conjunto). Além disso, um sistema híbrido pode não requerer um conjunto muito grande a ser mantido (devido a utilização da própria matriz estática), possibilitando a diminuição do custo computacional envolvido;
    \item Sistemas híbridos podem representar o acoplamento entre sistemas síncronos (i.e., que consideram o tempo como dimensão, como o 4DVar) com o filtro de Kalman por conjunto, ou ainda um conjunto de realizações dos sistemas 3DVars ou 4DVars. Ou seja, os sistemas híbridos introduzem uma alternativa para a modelagem de erros não Gaussianos.
\end{enumerate}

Um dos maiores desafios para a assimilação de dados é a especificação da matriz de covariâncias dos erros de previsão. As técnicas sequenciais e variacionais, como a IO, o 3DVar e o 4DVar, preconizam que a matriz de covariâncias dos erros de previsão seja pré-determinada, sendo esta uma característica estabelecida pela metodologia utilizada nestes sistemas. Por outro lado, os sistemas baseados no filtro de Kalman por conjunto, preconizam que a matriz de covariâncias dos erros de previsão é determinada com base nas diferenças entre os membros do conjunto e a sua média. Esta é uma grande vantagem destes sistemas em comparação com os demais. Porém, um conjunto infinito de previsões seria necessário para que se possa acessar toda a variabilidade atmosférica e então se amostrar todos os graus de liberdade necessários para a construção de uma matriz de covariâncias completa. Além disso, toda a estrutura relacionada ao tratamento das observações e (e.g., operadores de observações de radiâncias, a determinação e a aplicação de injunções (i.e., \textit{constraints}), fazem com que esses métodos sejam mais vantajosos pela sua infraestrutura.

Diante deste fato, ao longo da evolução das técnicas de assimilação de dados, inúmeros desenvolvimentos foram realizados com o objetivo de se amostrar de forma satisfatória as quantidades necessárias para se ponderar as contribuições das previsões na determinação das análises. As seções a seguir apresentam as técnicas mais comuns em assimilação de dados meteorológicos e como é feito o tratamento das covariâncias dos erros de previsão em cada uma delas.

\section{Revisão Bibliográfica}
\label{rev_biblio}

\subsection{Análise Objetiva} 
\label{anl_obj}

O emprego de análise objetiva em assimilação deu-se no início da década de 1950, em que eram utilizados polinômios para realizar a interpolação local ou global de dados observacionais com o objetivo de se inicializar um modelo numérico de previsão de tempo. 

Até 1954 os primeiros experimentos em PNT foram realizados por meio da interpolação manual e empírica das observações em ponto de grade, com a finalidade de se prognosticar o tempo a partir de informações previamente obtidas e as poucas observações de tempo disponíveis. Esta atividade compreendia um conjunto bastante reduzido de técnicas e era bastante dependente da habilidade e conhecimento do meteorologista. A técnica ficou conhecida como análise subjetiva. As primeiras tentativas de se produzir uma análise objetiva deu-se, no entanto, no final de década de 1940. \citeonline{panofsky/1949} deduziu um polinômio interpolador global (i.e., o mesmo polinômio era utilizado em toda a grade a ser analisada). Já na década de 1950, \citeonline{gilchristecressman/1954} utilizando as ideias iniciais de Panofsky, desenvolveram uma versão local de um polinômio interpolador, o qual podia ser definido para cada ponto de grade a ser analisado. Mas mesmo com estes avanços que permitiram a evolução da análise subjetiva para a análise objetiva, ainda restava o seguinte problema: como automatizar o procedimento de análise objetiva a fim de torná-lo operacional? Neste caso, o sentido da palavra operacional, tem o seguinte significado: baixo tempo de computação e erro comparável com a da análise subjetiva.

A partir de 1950, vários fatos importantes ocorreram. O ano de 1954 marca, por exemplo, o lançamento do primeiro computador de uso comercial da \textit{International Business Machines} (IBM, modelo 704). Em seguida, o ano de 1955 é marcado com início da PNT operacional. Além disso, há também a publicação do trabalho de \citeonline{bergthorssonedoos/1955} em que os autores agregam à recém formulada análise objetiva os elementos necessários para torná-la operacional. O método de análise objetiva espacial de \citeonline{bergthorssonedoos/1955} permite automatizar o processo de análise objetiva de forma rápida e eficiente (se comparada com a análise subjetiva convencional) e este método, foi o precursor do método de Correções Sucessivas (CS).

O método proposto por \citeonline{bergthorssonedoos/1955} representou um grande avanço para as técnicas da época e ajudou a cunhar as principais caraterísticas que hoje estão presentes na assimilação de dados moderna. Estas características incluem a utilização de uma previsão anterior a análise, fosse uma previsão, uma climatologia ou a combinação de ambos. No método proposto, os pesos atribuídos as previsões e as observações é empírico e dado em função da distância entre o ponto de grade analisado e ponto da estação (i.e., o ponto em que a observação está situada). Segundo \citeonline{daley/1996}, na formulação do método proposto \citeonline{bergthorssonedoos/1955} são feitas as seguintes considerações: as previsões e as observações contém erros e estes são não correlacionados; o erro da previsão é homogêneo (i.e., igual em todos os pontos de grade) e consequentemente a sua variância é independente do local; o erro da observação é espacialmente não correlacionado (i.e., cada estação tem o seu próprio erro) e este é dado apenas em função do erro do instrumento de medição. De forma geral, a formulação proposta possui o seguinte aspecto:

\begin{equation}
\label{eq:bd1955}
    f_{A}(\mathbf{r}_i) = \frac{E_{B}^{-2}f_{B}(\mathbf{r}_i)+E_{O}^{-2}(k)w(\mathbf{r}_k-\mathbf{r}_i)[f_{B}(\mathbf{r}_i)+f_{O}(\mathbf{r}_k)-f_{B}(\mathbf{r}_k)]}{E_{B}^{-2}+E_{O}^{-2}(k)w(\mathbf{r}_k-\mathbf{r}_i)}
\end{equation}

onde:

\begin{itemize}
    \item $f_{A}$ é o valor obtido pelo procedimento de análise objetiva no ponto de grade;
    \item $f_{B}$ é o valor do campo previsto (anterior a observação) no ponto de grade;
    \item $f_{O}$ é o valor da observação no ponto da estação;
    \item $\mathbf{r}_i$ é a posição do ponto de grade;
    \item $\mathbf{r}_k$ é a posição do ponto da estação;
    \item $w$ é o peso;
    \item $E_{B}$ é a variância do erro da previsão (homogêneo);
    \item $E_{O}$ é a variância do erro da observação (espacialmente não correlacionado).
\end{itemize}

Na Eq. \ref{eq:bd1955} estão relacionadas previsões (ou uma combinação destas com alguma climatologia) anteriores a análise e as observações, com seus pesos atribuídos em função da distância entre os pontos de grade das previsões e os pontos de estação das observações. Neste equação, é importante notar as seguintes propriedades: quando $\mathbf{r}_{k}=\mathbf{r}_{i}$ (i.e., quando os pontos de grade e da estação coincidem), então $w(\mathbf{r}_{k}-\mathbf{r}_{i})=1$. De outra forma, quando $|\mathbf{r}_{k}-\mathbf{r}_{i}|\rightarrow inf$ (i.e., quando a distância entre os pontos aumenta), então $w(\mathbf{r}_{k}-\mathbf{r}_{i}) \rightarrow 0$.

Com os desenvolvimentos da análise objetiva apontando para um futuro operacional, \citeonline{cressman/1959} estabelece o primeiro sistema de análise objetiva operacional nos Estados Unidos no \textit{Joint Numerical Weather Prediction Unit} (JNWPU, o centro de pesquisas em PNT precedente ao \textit{National Meteorological Center} (NMC, atual NCEP). O sistema operacional planejado por Cressman foi baseado no trabalho de \citeonline{bergthorssonedoos/1955} e neste trabalho o termo \textit{first guess} é utilizado para designar um ``chute inicial'', ou uma previsão de curto prazo. Neste sistema, as correções (das previsões de curto prazo a partir das observações) são feitas com as previsões interpoladas nos pontos das estações de forma que estas correções são feitas por algumas varreduras, as quais são mais suaves a cada nova varredura. O domínio é esférico e centrado na América do Norte e considera um raio de influência variável em que a distância entre as estações e o ponto de grade é considerada como peso.

Os métodos de análise objetiva evoluíram ao longo do Século XX, e acompanharam a evolução do sistema de observação terrestre. Com a disponibilidade das observações remotas da Terra a partir dos anos 1950, os métodos de assimilação de dados passaram também a incluir outros tipos de observações, que não apenas as observações geofísicas (e.g., temperatura, umidade, pressão etc). Eventualmente, observações de \textit{retrievals} (i.e., perfis recuperados) passaram a incrementar o conjunto de dados de observações a serem assimiladas e em seguida, dados de radiâncias (observações de satélite) também passaram a fazer parte desse conjunto de dados. O método variacional representa esta evolução \cite{sasaki/1958,sasaki/1970} nos métodos e técnicas e o seu estabelecimento promoveu o desenvolvimento de algorítmos de ingestão desses dados. Portanto, dentre todos os métodos de assimilação de dados existentes, a estrutura agregada ao método variacional permite não apenas a assimilação de uma grande variedade de dados de observação meteorológica, mas permite também o desenvolvimento de operadores de observação não lineares, os quais são fundamentais para a assimilação dos dados não convencionais. 

\subsection{Assimilação de Dados por Conjuntos}

Com o avanço da tecnologia, o computador eletrônico tornou-se cada vez mais acessível e novas possibilidades começaram a ser exploradas no campo da PNT. No final da década de 1960 e início da década de 1970, \citeonline{epstein/1969} e \citeonline{leith/1974} perceberam que, dada a natureza caótica da atmosfera apontada por Lorenz e o seu limite de previsibilidade, ao invés de se calcular deterministicamente apenas uma previsão de tempo, seria possível calcular um conjunto delas e, a partir deste conjunto, extrair um conjunto de momentos estatísticos, através dos quais seria possível construir a distribuição de probabilidades das incertezas das condições iniciais. Neste sentido, com um conjunto infinito de previsões perturbadas, seria possível descrever a incerteza da análise (condição inicial). Com isso, iniciaram-se os estudos da modelagem numérica por conjunto. 

Epstein realizou os primeiros estudos com previsão estocástico-dinâmica, os quais consideravam explicitamente a incerteza das previsões dos modelos e o desenvolvimento de técnicas que consideravam a construção de distribuições de probabilidades com base na estimativa dos momentos estatísticos fornecidos pelos próprios modelos dinâmicos. No entanto, mesmo para modelos com poucos graus de liberdade, esta aproximação para a determinação de todos os momentos estatísticos de uma distribuição de probabilidades mostrou-se computacionalmente cara, e então uma nova aproximação foi tentada, desta vez, considerando apenas dois momentos estatísticos: média e covariância. Então, percebeu-se que com apenas estes dois momentos estatísticos era possível construir-se uma distribuição de probabilidades que representasse as incertezas da condição inicial. Em seguida, na década de 1970, Leith mostrou que as previsões obtidas com a técnica de Epstein eram válidas apenas se a distribuição de probabilidades gerada pelo conjunto fosse uma amostra representativa da distribuição de probabilidades relacionadas a atmosfera. Com isso, Leith propôs a utilização de um conjunto de membros - ao invés de se utilizar apenas um único membro, como em uma única previsão determinística. A proposta de Leith empregava a técnica de Monte Carlo, em que a solução é aproximada de forma exaustiva por meio de repetidas simulações aproximadas do fenômeno. Esta similaridade com o aspecto da ideia da previsão de tempo por conjunto possibilitou a adequação da técnica para gerar um conjunto de previsões independentes. 

Ao mesmo tempo que Epstein e Leith desenvolviam as bases da previsão dinâmico-estocástica, e cujas ideias foram posteriormente utilizadas por, e.g., \citeonline{hoffmanekalnay/1983} no desenvolvimento da técnica de \textit{Lagged Average Forecasting} (LAF), surgia o filtro de Kalman-Bucy \cite{kalmanebucy/1961}. O filtro de Kalman-Bucy, ou simplesmente filtro de Kalman, é um algoritmo desenvolvido com o objetivo de se utilizar medições ruidosas observadas ao longo do tempo e realizar estimativas de estados futuros do modelo do sistema observado. Estas estimativas são melhores do que aquelas que seriam produzidas utilizando-se apenas uma única medição observada no tempo (no sentido Bayesiano). O filtro de Kalman é um algoritmo aplicado em problemas tipicamente lineares e foi empregado com sucesso em diversos desenvolvimentos associados a exploração espacial, indústria aeronáutica e militar. Uma principais características do filtro de Kalman linear está na estimativa dos momentos estatísticos, os quais são utilizados na predição dos estados do sistema observado, e que são fundamentais na determinação das relações entre os estados do sistema observados, i.e., a estimativa das variâncias e das covariâncias dos estados do sistema observados.

Tais características do filtro de Kalman o tornaram atrativo para aplicações em problemas com dinâmica não linear, em que a estimativa das covariâncias é necessária para a determinação do estado. O filtro de Kalman por conjunto (\textit{Ensemble Kalman Filter} - EnKF, \citeonline{evensen/1994}), foi então desenvolvido como uma simplificação do filtro de Kalman original, em que as covariâncias são então atualizadas utilizando os próprios estados do sistema, ao invés de um modelo para a sua representação. Nas Seções \ref{sec:enkf} e \ref{sec:ensrf} do Capítulo \ref{cap:dados_metodologias}, será apresentada uma introdução mais detalhada de dois tipos de filtros de Kalman por conjunto.

\subsection{Assimilação de Dados Híbrida}
\label{tec_hib_assim_dados}

Combinando-se as características de dois ou mais sistemas de assimilação de dados, pode-se obter o que se chama de sistema híbrido. Um sistema híbrido, é então um sistema cuja solução ou é variacional, ou é sequencial, mas que apresenta entre suas componentes, uma combinação - em geral linear, de dois ou mais parâmetros. O exemplo mais comum de sistema híbrido em aplicação na assimilação de dados atmosféricos, são os sistemas por conjunto-variacionais, em que uma combinação linear entre as matrizes de covariâncias do filtro de Kalman por conjunto e do sistema 3DVar (ou 4DVar) é feita.

Os primeiros trabalhos a trazerem a tona esta nova abordagem para o tratamento do problema de análise meteorológica atmosférica, é o trabalho de \citeonline{hamillesnyder/2000} no qual os autores propuseram um sistema híbrido entre o \textit{Ensemble Kalman Filter} (EnKF) e o 3DVar utilizando um modelo quasi-geostrófico perfeito (ou seja, considerando a climatologia do modelo como a verdade). \citeonline{ethertonebishop/2004} testaram a resistência do sistema híbrido proposto por \citeonline{hamillesnyder/2000} utilizando o mesmo modelo quasi-geostrófico perfeito e fizeram testes com um sistema híbrido semelhante utilizando um \textit{Ensemble Transform Kalman Filter} (ETKF) aplicado ao mesmo modelo. Posteriormente, outros autores também realizaram testes com este tipo de sistema híbrido \cite{wangetal/2008a,wangetal/2008b,elakkraouietodling/2013} e desenvolveram também outros tipos de híbridos com o objetivo de se acessar características diferentes dos sistemas envolvidos \cite{buehner/2005,zupanski/2005,wangetal/2007,zhangetal/2009,leietal/2012,claytonetal/2012,todlingeelakkraoui/2013}.

Como pode-se observar, diversos testes foram realizados com diferentes modelos e diferentes esquemas de assimilação de dados. Modelos completos e simplificados e técnicas variacionais de 3 e 4 dimensões, em conjunto com sistemas sequenciais como o filtro de Kalman por conjunto e a Interpolação Ótima, geraram um conjunto de aproximações para a solução do problema de análise atmosférica em que o senso determinístico é alcançado através de técnicas - em sua maioria estatísticas, como o filtro de Kalman. Esta combinação tem se mostrado bastante promissora porque permite que os sistemas em conjunto, amenizem suas deficiências mútuas. Com isso, é notável que alguns sistemas híbridos apresentem como resultado a melhoria da análise variacional (e.g., sistemas do tipo EnKF-Var, ETKF-Var, \citeonline{hamillesnyder/2000,ethertonebishop/2004} respectivamente) e outros a melhoria da análise do sistema por conjunto (e.g., \textit{Hybrid Nudging Ensemble Kalman Filter} - HNEnKF, \citeonline{leietal/2012}). 

Uma das principais características dos sistemas híbridos é que os seus sistemas componentes podem ser melhorados mutuamente. Desta forma, pode-se utilizar as covariâncias dos erros de previsão (ou modelagem) provenientes de algum tipo de EnKF para melhorar a especificação das covariâncias na minimização da Função Custo ($J$) de um sistema variacional, em que estão relacionados os vetores de estados do modelo e observações e as suas respectivas matrizes de erros (e.g., Eq. \ref{eq:fcusto}). Por outro lado, sabe-se que o sistema por conjunto possui posto (matricial) incompleto (\textit{rank deficient}) e que a representação das covariâncias dos erros de previsão dos sistemas variacionais possui posto completo (\textit{full rank}, ou seja, contém todos os graus de liberdade definidos). Logo, o sistema variacional pode melhorar o ciclo de assimilação de dados de um sistema baseado no filtro de Kalman por conjunto, pois, do contrário, para que o conjunto possua posto completo, esse deve ser infinito. Como consequência disto, a resposta entre as análises e as previsões provenientes dos dois sistemas, pode melhorar o sistema variacional. Além disso, o filtro de Kalman por conjunto, resolve um problema linear, enquanto que a minimização de uma função custo variacional é um problema não linear (a função custo variacional, 3D ou 4D é uma função quadrática). Este cenário favorece o conjunto, pois a previsão fornecida para o sistema por conjunto possui como características estas relações não lineares, que são impressas na análise e propagadas pelo modelo de PNT.

\citeonline{hamillesnyder/2000} foram os primeiros a propor um sistema híbrido. Neste trabalho os autores propuseram um sistema híbrido entre o EnKF e o 3DVar utilizando um modelo quasi-geostrófico perfeito (ou seja, quando a climatologia do modelo é considerada como verdade). Neste híbrido, os autores calcularam uma matriz de covariâncias dos erros de previsão (matriz $\mathbf{B}$) utilizando a Eq. \ref{eq:10} (as equações abaixo foram modificadas a partir dos seus respectivos originais para facilitar a comparação com as equações que serão utilizadas no trabalho de tese).

\begin{equation}
\label{eq:10}
\mathbf{B}=(1-\alpha)\mathbf{P}^{b}+\alpha\mathbf{SCS}^{T}
\end{equation}

Esta equação representa uma combinação linear entre a matriz de covariâncias do conjunto $\mathbf{P}^{b}$ e um modelo de covariâncias ``estático'' dado por $\mathbf{SCS}^{T}$ (em que $\mathbf{S}$ é um operador que transforma do espaço espectral para ponto de grade e $\mathbf{C}$ uma matriz com as variâncias dos coeficientes espectrais), em que o peso $\alpha$ é atribuído a cada uma das partes. \citeonline{ethertonebishop/2004} testaram a resistência do sistema híbrido proposto por \citeonline{hamillesnyder/2000} utilizando o mesmo modelo quasi-geostrófico perfeito e fizeram testes com um sistema híbrido semelhante utilizando um ETKF aplicado ao mesmo modelo. A matriz de covariâncias utilizada pelos autores neste trabalho  é a Eq. \ref{eq:11} a seguir: 

\begin{equation}
\label{eq:11}
\mathbf{B}=(1-\alpha)\lambda\mathbf{P}^{b}+\alpha\rho\mathbf{B}_{3dvar}
\end{equation}

A Eq. \ref{eq:11} é semelhante aquela utilizada por \citeonline{hamillesnyder/2000}, mas utilizando uma matriz de covariâncias estática $\mathbf{B}_{3dvar}$ de um sistema variacional de 3 dimensões e com a inclusão dos coeficientes $\lambda$ e $\rho$, que também são pesos (assim como $\alpha$) e possuem a função de capturar as contribuições de cada uma das parcelas da combinação linear. Estes pesos são calculados a cada ciclo de assimilação de dados e refletem as influências que as matrizes $\mathbf{P}^{b}$ e $\mathbf{B}_{3dvar}$ exercem sobre a matriz $\mathbf{B}$ calculada. \citeonline{buehner/2005} em um estudo sobre a modelagem das covariâncias dos erros de previsão em um sistema de PNT quasi-operacional, avaliou as covariâncias dos erros de previsão geradas pelo sistema híbrido de \citeonline{hamillesnyder/2000} em comparação com resultados obtidos com outras técnicas utilizando o filtro e Kalman por conjunto e utilizando técnicas variacionais. \citeonline{zupanski/2005} desenvolveu o \textit{Maximum Likelihood Ensemble Filter} (MLEF), permitindo que um operador não linear fosse utilizado na minimização da função custo variacional, sendo este realizado no espaço do conjunto. Esta abordagem permite, por exemplo, a inclusão de dados de precipitação na assimilação de dados por conjunto. Esta é uma abordagem interessante porque os erros associados a precipitação não possuem distribuição Gaussiana, enquanto que, em geral, os erros de previsão são considerados com distribuição Gaussiana. \citeonline{wangetal/2007} propuseram um sistema híbrido combinando o ETKF e a técnica de Interpolação Ótima (\textit{Optimum Interpolation} - OI) e fizeram comparações com o esquema \textit{Ensemble Square Root Filter} (EnSRF). A matriz de covariâncias utilizada pelos autores neste estudo (Eq. \ref{eq:12}) também é semelhante aquela utilizada por \citeonline{hamillesnyder/2000} e por \citeonline{ethertonebishop/2004}, mas com a diferença de que o método de assimilação de dados (para a parte estática do sistema híbrido) foi o método de Interpolação Ótima:

\begin{equation}
\label{eq:12}
\mathbf{B}=(1-\alpha)\mathbf{P}^{b}+\alpha\mathbf{B}_{IO}
\end{equation}

\citeonline{wangetal/2008a} e \citeonline{wangetal/2008b}, com base no sistema híbrido elaborado por \citeonline{hamillesnyder/2000}, propuseram um sistema híbrido para o modelo WRF (um modelo de PNT de física completa) definindo um novo incremento de análise através de uma extensão da variável de controle, uma técnica originalmente proposta por \citeonline{lorenc/2003}. A matriz de covariâncias utilizada pelos autores (Eq. \ref{eq:13}) utiliza uma abordagem um pouco diferente, por utilizar um produto Schur entre a matriz de covariâncias do conjunto e uma matriz de correlação, cuja função é localizar o novo incremento de análise.

\begin{equation}
\label{eq:13}
\mathbf{B}=(1-\alpha)\mathbf{B}_{3dvar}+\alpha\mathbf{P}^{b}\circ\mathbf{C}
\end{equation}

\citeonline{zhangetal/2009} elaboraram um acoplamento entre o EnKF e o sistema 4DVar (denominado E4DVAR) aplicado ao modelo Lorenz96. Neste estudo, os autores também definiram uma matriz de covariâncias utilizando uma combinação linear entre as matrizes de covariâncias do EnKF e do 4DVar (Eq. \ref{eq:14}).

\begin{equation}
\label{eq:14}
\mathbf{B}=\alpha\mathbf{P}^{b}+(1-\alpha)\mathbf{B}_{4dvar}
\end{equation}

\citeonline{leietal/2012} desenvolveram um sistema híbrido baseado na técnica de \textit{Nudging} e EnKF (denominado HNEnKF) aplicado também ao modelo Lorenz96. \citeonline{claytonetal/2012} avaliaram a implementação operacional de um sistema híbrido utilizando o sistema de previsão por conjunto do MetOffice (\textit{MetOffice Global and Regional Ensemble Prediction System} - MOGREPS, baseado no ETKF) e o sistema 4DVar aplicado ao modelo global operacional do centro. A matriz de covariâncias utilizada nesta implementação (Eq. \ref{eq:15}) também inclui o produto Schur entre a matriz de covariâncias do ETKF e uma matriz de correlação $\mathbf{C}$ que tem a função de localizar correlações muito pequenas provenientes do conjunto e que podem ser dominadas por erros de amostragem. Na Eq. \ref{eq:15}, os coeficientes $\alpha_{c}^{2}$ e $\alpha_{e}^{2}$ são os pesos dados as matrizes $\mathbf{B}_{c}$ (do 4DVar) e $\mathbf{B}_{e}$ (Eq.\ref{eq:15.1}, do ETKF) respectivamente:

\begin{align}
\label{eq:15}
\mathbf{B} & = {} \alpha_{c}^{2}\mathbf{B}_{c}+\alpha_{e}^{2}\mathbf{B}_{e} \\
\label{eq:15.1}
\mathbf{B}_{e} & = {} \mathbf{P}^{b} \circ \mathbf{C}
\end{align}

Mais recentemente, \citeonline{elakkraouietodling/2013} estudam um sistema híbrido sem a necessidade de se empregar um sistema de análise por conjunto baseado em filtro de Kalman. \citeonline{wangelei/2013} estudam uma extensão da técnica aplicada por \citeonline{wangetal/2008a,wangetal/2008b} (Variável de Controle Estendida) e realizam um sistema híbrido entre um filtro de Kalman por conjunto e o 4DVar (formando um 4DEnsVar) utilizando as previsões do modelo GFS. A grande vantagem deste acoplamento é utilizar as covariâncias do conjunto para evitar a necessidade de se utilizar o modelo tangente linear para propagar as covariâncias no tempo e assimilando as observações no tempo correto.

\subsection{Modelagem de Covariâncias}
\label{mod_covars}

Como o problema de se especificar as covariâncias para uso na minimização de uma função custo variacional ou para aplicação direta em um sistema mais simples com poucos graus de liberdade, foi necessário desenvolver-se modelos matemáticos capazes de descrever a natureza não linear das relações multivariadas das quantidades de interesse. Ao longo desse desenvolvimento, diferentes técnicas foram propostas, e estas técnicas podem ser divididas em basicamente, dois grupos: i) técnicas que calculam explicitamente as covariâncias, utilizando algum tipo de metodologia paramétrica; ii) técnicas que amostram a incerteza de um determinado conjunto de informações utilizando um procedimento do tipo Monte Carlo.

Sistemas híbridos de assimilação de dados utilizando um filtro de Kalman por conjunto e uma sistema variacional foram introduzidos no início dos anos 2000 \cite{hamillesnyder/2000,lorenc/2003,zupanski/2005} e tem sido aplicados com ênfase na amostragem e na representação das variações diárias das covariâncias dos erros de previsão, introduzindo os chamados ``erros do dia'' \cite{corazzaetal/2003} a parte estática das covariâncias. A representação das covariâncias dos erros de previsão é um dos principais problemas a serem endereçados na assimilação de dados operacional e a sua determinação em sistemas determinísticos (e.g., sistemas variacionais) é de grande importância.

Sistemas variacionais como o 3DVar utilizam uma matriz estática para representar as covariâncias dos erros de previsão. Isto significa que estas covariâncias - embora tenham sido especificadas com base em previsões distribuídas ao longo do tempo, elas não variam em conjunto com a evolução temporal das análises e previsões e são, portanto, fixas durante todo o processo de assimilação de dados. Uma situação similar ocorre com o 4DVar em que estas mesmas covariâncias são utilizadas entre as janelas de assimilação de dados, embora dentro desta janela de assimilação, as covariâncias são propagadas no tempo durante a assimilação das observações utilizando a versão tangente linear do modelo de previsão. As diferenças na forma como as estatísticas de erro variam com o tempo de acordo com as variações no fluxo atmosférico levam ao que se chama de ``dependência de fluxo''. Anisotropia é uma característica relacionada com o aspecto geométrico das funções de correlação que fazem com que as covariâncias se ajustem aos gradientes impressos no fluxo atmosférico (e.g., como em uma zona frontal). Inomogeneidade é uma outra característica que está relacionada com a distribuição espacial das covariâncias em cada ponto de grade e como as funções de correlação são definidas. Estas características inerentes a representação das covariâncias são atingidas quando as estruturas de covariâncias são modeladas levando-se em consideração a natureza física da dinâmica da atmosfera.

Covariâncias dependentes do fluxo atmosférico são a característica principal dos sistemas modernos de assimilação de dados e sua determinação, no início ou durante a janela de assimilação de dados, permite que a análise a ser produzida contabilize a dependência temporal das inovações trazidas pelas observações nos incrementos de análise, nas direções horizontal e vertical. Além disso, características desejáveis das covariâncias dos erros de previsão também incluem a anisotropia e a inomogeneidade (i.e., estatísticas não homogêneas - \citeonline{rabier/2005}). A dependência do fluxo atmosférico refere-se as alterações da dinâmica da atmosfera e suas variações com o tempo, as quais devem ser representadas dentro da matriz de covariâncias. Anisotropia e inomogeneidade são características da distribuição espacial das covariâncias e são governadas por funções de correlação espaciais. Funções de correlação são modeladas de forma a permitir que as covariâncias sejam não homogêneas (i.e., diferentes entre os pontos de grade).

Vários esforços já foram feitos de forma a conferir algum grau de anisotropia e dependência de fluxo as covariâncias estáticas. O desenvolvimento das técnicas variacionais para uso operacional durante os anos 1990, incluiu modificações na estrutura da assimilação de dados variacional. \citeonline{desroziers/1997} introduziu uma transformação de coordenadas para permitir que sistemas de assimilação de dados pudessem contabilizar as estruturas de sistemas frontais. Este desenvolvimento proporcionou a produção de uma análise com covariância fluxo-dependente e correlações anisotrópicas.

Os gradientes representados no fluxo atmosférico também incluem importantes informações que podem ser utilizadas para se determinar a anisotropia. Os gradientes espaciais das previsões podem, então, serem utilizados para modelar as estatísticas dos erros. \citeonline{riishojgaard/1998} mostrou como utilizar as alterações no campo de umidade para modelar funções de correlação que podem ser ``esticadas'' para acomodar as covariâncias de acordo com os gradientes nas previsões. Outras aproximações para a modelagem de funções de correlação que implicam em covariâncias fluxo-dependentes, envolveram a aplicação do método de ondeletas em conjunto com o método espectral \cite{fisher/2003}. Neste caso, dificuldades surgem com a especificação das ondeletas na esfera. Filtros recursivos são também um outro método conhecido para se derivar formas (de funções) quasi-Gaussiana, que podem ser utilizadas na modelagem de covariâncias. Este método tem sido utilizado com sucesso em várias aplicações; \citeonline{haydenepurser/1995} aplicaram filtros recursivos isotrópicos para o pré-processamento do \textit{National Environmental Satellite and Data Information Service} (NESDIS); recentemente, filtros recursivos foram também aplicados ao sistema GSI para modelar a aplicação das covariâncias dos erros de previsão \cite{wuetal/2002}.

Dentre estes desenvolvimentos, outros métodos tem sido desenvolvidos e estão mais relacionados com a representação de não-linearidades do fluxo atmosférico nas estatísticas dos erros de previsão. O 4DVar \cite{thepautandcourtier/1991} é uma extensão do 3DVar \cite{lorenc/1986} e pode implicitamente evoluir as covariâncias dentro de uma janela de assimilação. \citeonline{corazzaetal/2003} adicionou a dependência do fluxo atmosférico as covariâncias contabilizando os ``erros do dia'' utilizando \textit{bred-vectors}. Métodos híbridos \cite{hamillesnyder/2000} foram introduzidos aproveitando as vantagens dos métodos de filtro de Kalman por conjunto. Esta nova metodologia apresenta várias vantagens, incluindo o fato de que muitos centros operacionais já experimentaram algum método variacional e tem alguma experiência também com filtros de Kalman por conjunto. Cada um destes métodos e metodologias podem ser tomados como um complemento para se melhorar as deficiências do outro \cite{wangetal/2007,wangetal/2009}. Recentemente, métodos híbridos tem sido aplicados em situações reais \cite{wangetal/2013,claytonetal/2012}, utilizando covariâncias estimadas a partir de um filtro de Kalman por conjunto em combinação com as covariâncias estáticas de um 3DVar.

\section{Objetivos}
\label{cap:objetivos}

Esta tese de doutorado insere-se no contexto dos avanços científicos realizados durante as duas últimas décadas (2000-presente) na área de assimilação de dados meteorológicos. Nesta área, os maiores desafios encontram-se na determinação das covariâncias dos erros de previsão e a sua aplicação para a determinação das análises (condições iniciais) dos modelos de PNT. 

Com a premissa de que o CPTEC é um dos maiores centros de PNT do Hemisfério Sul e em consonância com a sua missão, esta tese de doutorado tenta responder ao seguinte questionamento: \textbf{qual é a contribuição das covariâncias do filtro de Kalman por conjunto na determinação das análises e previsões de um sistema de assimilação de dados global?}

Partindo-se do princípio de que existe a necessidade de se aprimorar a forma como as covariâncias globais são tratadas no sistema de assimilação de dados em uso no CPTEC, propõem-se um aprimoramento para o atual sistema de forma a incluir os ``erros do dia'' dentro da etapa de cálculo da análise atmosférica.

\subsection{Objetivo Geral}
\label{obj_geral}

Obter um conjunto de análises a partir da minimização de uma função custo variacional tridimensional, utilizando uma combinação de covariâncias estacionárias e dinâmicas.

\subsection{Objetivos Específicos}
\label{obj_espec}

Para se obter um conjunto de análises calculadas com base na combinação de covariâncias estacionárias e dinâmicas, as seguintes tarefas devem ser cumpridas:

\begin{itemize}
    \item Calcular uma nova matriz de covariâncias, baseada na versão do modelo atmosférico utilizado;
    \item Identificar as principais estruturas associadas as covariâncias dos erros de previsão do modelo de circulação atmosférico do CPTEC;
%    \item Investigar o impacto da nova matriz de covariâncias nas previsões de curto prazo do modelo;
    \item Habilitar o sistema de assimilação de dados GSI (componente do sistema G3DVAR) para ler as covariâncias de um filtro de Kalman por conjuntos;
    \item Estabelecer uma rotina de realizações cíclicas do sistema híbrido habilitado;
    \item Investigar os efeitos da atualização cíclica das covariâncias híbridas nas análises e previsões.
\end{itemize}

\section{Estrutura do Documento}
\label{sec:estrutura_doc}

Para se endereçar os objetivos propostos da Seção \ref{cap:objetivos}, este documento foi organizado da seguinte maneira: no Capítulo \ref{cap:dados_metodologias}, são apresentadas as metodologias e os dados utilizados no estudo. A descrição do filtro de Kalman por conjunto utilizado, está detalhado em duas seções do capítulo, bem com uma descrição do modelo de circulação geral do CPTEC e o sistema de assimilação de dados variacional. A metodologia utilizada para o cálculo da matriz de covariâncias estática está destacada e detalhada no Capítulo \ref{cap:matriz_cov_estatica}. A metodologia para a incorporação das covariâncias do conjunto na estrutura variacional do sistema de assimilação de dados bem como a descrição do ciclo de assimilação de dados, estão também destacados e são apresentados no Capítulo \ref{cap:incorpora_covars}. No Capítulo \ref{cap:resultados} são apresentados os resultados, divididos em duas partes: na primeira parte, são apresentados os resultados obtidos com a aplicação da matriz de covariâncias estática e o seu impacto na assimilação das observações e nas previsões de curto prazo; na segunda parte, são apresentados os resultados obtidos com a aplicação das covariâncias do conjunto em combinação com as covariâncias estáticas dentro da estrutura variacional, e o seu impacto nas análises e previsões até 5 dias.

Foram adicionados ao documento principal da tese, 4 apêndices que trazem resultados da teoria variacional e a aplicação da matriz de covariâncias na estrutura variacional. Estes documentos são registros de estudos prévios sobre os sistemas envolvidos no trabalho e foram adicionados para dar suporte ao entendimento de algumas seções. Com os resultados obtidos e, dentro do tempo disponível para realizar o trabalho, foram elaborados dois artigos científicos. O primeiro - submetido para a Revista Brasileira de Meteorologia, traz alguns resultados e contribuições ao entendimento da importância e aplicação das covariâncias na assimilação de dados. O segundo artigo - submetido para revista \textit{Journal of Applied Meteorology and Climatology}, traz os resultados obtidos com o sistema híbrido e contribui para o entendimento na forma como as covariâncias híbridas são aplicadas. Estes dois artigo encontram-se nos dois últimos apêndices do documento principal.